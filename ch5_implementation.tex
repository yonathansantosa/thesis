\chapter{Implementation}
\label{chap:implementation}

\section{Preparation}
    \subsection{Dataset Preparation}
        \subsubsection{Character Embedding}
            The character dictionary consists of first 128 ASCII
            characters with deletion for non character types (the
            first 32 ASCII symbols). Then the character embedding was
            initialized by randomizing from normal distribution with
            $\mu = 0$ and $\sigma = 1$. Another entries such as
            unkonwn token \textit{\textless unk\textgreater}, starting
            token \textit{\textless s\textgreater}, ending token
            \textit{\textless e\textgreater} and padding token
            \textit{\textless p\textgreater} were added along with the
            random character embedding initialization.

        \subsubsection{Pretrained Word Embedding}
            In order to train the model, pretrained embedding is
            needed since the model will tries to predict embedding
            from known vocabulary $v \in \mathcal{V}$ with its known
            embedding $w \in \mathcal{W}$. For this purpose, word2vec
            which trained on Google news dataset using skip-gram model
            containing 3 million words and phrases (name, hyperlink,
            connected words, etc.) was used
            \citep{Distributed2013mikolov}. Word2vec
            \footnote{Pretrained embedding available at 
            \url{https://code.google.com/archive/p/word2vec/}}
            contains phrases from negative sampling and for the purpose of
            this research those phrases did not included. The embedding
            has 300 dimensional vectors. Only first 40 thousands words with
            removal if the word contains "\_" (underscore) or "http".
            
            Another pretrained embedding is
            polyglot\footnote{Pretrained embedding available at
            \url{https://polyglot.readthedocs.io/en/latest/index.html}}
            \citep{polyglot2013alrfou} which contains multilingual
            embeddings. For this research, only English embedding that
            contains around 100 thousands with 60 dimensional vector
            representation. This pretrained embedding is also used in OOV
            handling model \textsc{Mimick} \citep{mimicking2017Pinter} which
            used as baseline model.

            Dict2vec\footnote{Pretrained embedding available at
            \url{https://github.com/tca19/dict2vec}} is yet another word
            embedding that trained based on the definition of a word in
            dictionary \citep{dict2vect2017tissier}. Originally, this
            embedding was tested using word similarity tasks, hence this
            embedding is used as baseline model for word similarity tasks.

        \subsubsection{Word Similarity Dataset}
            Several word similarity dataset were used in order to
            increase the pair examples since for the datasets that is
            collected, the highest number of pairs is just above 3000
            pairs. Those datasets used for word similarity tasks are
            Card-660 \citep{card660:pilehvar-etal:2018}, MC-30
            \citep{mc30:strongContextualHypothesis}, MEN-TR-3k
            \citep{mentr3k:bruni-etal-2012-distributional}, MTurk-287
            \citep{mturk287:Radinsky:2011:WTC:1963405.1963455},
            Mturk-771
            \citep{mturk771:Halawi:2012:LLW:2339530.2339751}, RG-65
            \citep{rg65:Rubenstein:1965:CCS:365628.365657},
            RW-STANFORD \citep{rw:luong-etal-2013-better}, SimLex-999
            \citep{simlex999:hill2014}, YP130
            \citep{yp130:inproceedings}, VERB143
            \citep{vp143:baker-etal-2014-unsupervised}, and Wordsim353
            \citep{wordsim353:2002:PSC:503104.503110}.

    \subsection{Programming Language and Tools}
        The model was trained using PyTorch 1.1.0 machine learning
        library \citep{pytorch2017paszke} on top of Python 3.6. Most
        of the basic functions, for instance 2d convolution layer, 2d
        maxpool layer, fully connected layer, bi-lstm, and many
        activation functions and loss function were already
        implemented as a library in PyTorch, thus will serve enough
        for the purpose of this research.
            
    \subsection{Hardware}
        The model was trained on the freely available Google
        Colaboratory\footnote{Google Colaboratory available at
        \url{https://colab.research.google.com/}} which gives
        randomized hardware specification, thus the exact hardware
        used cannot be determined. The GPU was used to train the
        model. Nevertheless, this only affects the time needed to
        train the model and not the results.

\section{Training}
    \subsection{Training OOV model}
        Firstly, the pretrained embeddings acted as the datasets were
        splitted up into train-val set with $80\%$ and $20\%$
        randomized split respectively with batch size of 64. The word
        $w_i \in V$ becomes the input of the model and the word
        embedding $e_i \in \mathcal{W}$ becomes the target. The input
        word splitted into sequence of characters and starting token
        and ending token were added at the beginning and at the end of
        the word respectively. 
        % For the proposed model, padding token was added at the
        % beginning and at the end of the word to make single
        % character entries at the beginning and at the end of the
        % word to be able to be processed by the CNN. 
        The sequence of characters then transformed into character
        embeddings then processed by the model producing the predicted
        word embedding. The model was trained with learning rate $lr =
        0.01$ with no dropout as dropout does not work well just as in
        \textsc{Mimick} \citep{mimicking2017Pinter}.
        \begin{table}[]
            \centering
            \caption{OOV Handling Model Parameters}
            \label{tab:hyperparameter}
            \begin{tabular}{@{}lcc@{}}
                \toprule
                \textbf{Hyperparameter} & \multicolumn{1}{l}{\textbf{\textsc{Mimick}}} & \multicolumn{1}{l}{\textbf{CNN}} \\ \midrule
                Learning Rate ($\eta$) & [0.01; 0.1] & 0.1 \\
                Batch size & 64 & 64 \\
                Epoch (word2vec) & 1000 & 1000 \\
                Epoch (polyglot-en) & 100 & 100 \\
                Epoch (dict2vec) & 1000 & 1000 \\
                Momentum & 0.5 & 0.5 \\
                Dropout & [0; 0.2] & 0.2 \\
                Num features & [100; 700] & 100 \\ \bottomrule
            \end{tabular}
        \end{table}

        For the proposed model, the character embeddings were
        processed with CNN n-gram following
        \cite{convolutional2014kim} architecture for word n-gram.
        N-gram with window size $n = [2, 3, 4, 5, 6, 7]$ were used
        with respective kernel size $n \times d$ to simulate n-grams.
        Different N-gram sizes, for instance $n = [2, 3, 4]$ and $n =
        [5, 6, 7]$. The results on the downstream tasks for the
        different settings then compared with the whole model and with
        the \textsc{Mimick} model.
        
        After max-over-time pooled, the vector representation then
        passed into fully connected network with highway network to
        produce the predicted word embedding. The error then
        calculated using Mean Squared Error as mentioned on equation
        \ref{eq:errorf} then back-propagated to update the weights.

        Similar datasets are used to train \textsc{Mimick}
        \cite{mimicking2017Pinter} with parameters taken from the
        original paper. After compared several parameters then
        adjusted to see whether there is improvement or not. The
        summary of the hyperparameters are as shown in table
        \ref{tab:hyperparameter}.

        % \begin{table}[]
        %     \centering
        %     \caption{OOV Handling Model Parameters}

        %     \begin{tabular}{@{}lcl@{}}
        %         \toprule
        %         \textbf{Hyperparameter} & \multicolumn{1}{l}{\textbf{\textsc{Mimick}}} & \textbf{CNN} \\ \midrule
        %         Learning Rate ($\eta$) & \multicolumn{2}{c}{0.1} \\
        %         Batch size & \multicolumn{2}{c}{64} \\
        %         Epoch (word2vec) & \multicolumn{2}{c}{1000} \\
        %         Epoch (polyglot-en) & \multicolumn{2}{c}{100} \\
        %         Epoch (dict2vec) & \multicolumn{2}{c}{1000} \\
        %         Momentum & \multicolumn{2}{c}{0.5} \\
        %         Num features & [100; 700] & \multicolumn{1}{c}{100} \\ \bottomrule
        %     \end{tabular}
        % \end{table}
    
\section{Evaluating with Downstream Tasks}
    \subsection{Part-of-Speech Tagging}
        For POStagging task, the readily brown corpus and its tagset
        from NLTK\footnote{Available at \url{https://www.nltk.org/}}.
        In this tasks, two kind of evaluation methods were done.
        Firstly, all the word embeddings only inferred from the
        trained OOV model and the OOV model trained together with the
        POStagger. Secondly, only OOV from the pretrained embedding
        are inferred by using the OOV model and then those embeddings
        are used as input for the POStagger. For the latter method,
        embeddings of the words are frozen to get a notion whether the
        OOV model improves the results compared to replacing OOV with
        unknown token or a random embedding. The OOV model then
        changed with \textsc{Mimick} \citep{mimicking2017Pinter} to
        compare the results with the previous state-of-the-art.

    \subsection{Word Similarity}
        Word similarity task is quite straight forward. Given pairs of
        words, if the word is an OOV in the pretrained embedding, the
        word embeddings were predicted from OOV models then the cosine
        similarity of the word embeddings are calculated and compared
        between two models, else the original embedding from the
        pretrained embedding is used. 
        
        The baseline embeddings used for this task, dict2vec
        \citep{dict2vect2017tissier}, was also tested using random OOV
        embedding by randomly giving OOV word random embedding and
        choosing the maximum results of several tries. The results
        then compared with if the OOV were predicted using the OOV
        models trained with dict2vec pretrained embedding
        \citep{dict2vect2017tissier}.