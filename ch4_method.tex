\chapter{Method}
\label{chap:method}

% This is the introduction to the thesis.\footnote{And this is a
% footnote.}  The conclusion is in Chapter on page
\section{Out-of-Vocabulary Model}
    \subsection{Sequence Feature Extraction}
        OOV problem was handled from quasi-generative perspective as
        aforementioned in chapter \nameref{chap:intro} by using neural
        language model under assumption that there is a form that
        could generate embedding for the original embedding. Hence,
        the original vocabulary and its embedding were used for
        training the model to generate the embedding. As mentioned in
        chapter \nameref{chap:intro} and chapter \nameref{chap:preliminaries}, 
        the reasons why \textsc{Mimick}
        could perform worse is because the OOV embedding is generated
        from the last hidden states of the bi-LSTM and the hidden
        states are controlled by cell gates $C_t$ making the
        information that is passed on is the most recent information.
        If at certain time step $t$ the cell gates decided to forget
        past information, then the early information might not be
        coded into the last hidden state. On top of that, there are
        evidences that recurrent architecture could perform worse than
        CNN for sequence modelling \citep{empirical2018shaujie}. 
        
        If explained formally, when $C_t = 0$ in LSTM from equation
        \ref{eq:lstm:C_t}, hidden state from equation
        \ref{eq:lstm:h_t} will also be $0$, resetting to its starting
        state, rendering hidden states prior to time $t$ gone. This
        problem can be solved by using bi-LSTM, since bi-LSTM
        processes sequence in forward and reverse order making both
        early and later sequences information is held by the last
        hidden state for each reverse LSTM and forward LSTM
        respectively. Another problem might arise when we need to
        divide the sequence into more than three subsequence as shown in
        figure \ref{fig:subsequence}. Hence another approach is needed
        since intermediate subsequence might get deleted even by using
        bi-LSTM. Another method that might be able to solve this
        problem for \textsc{Mimick} is by increasing the hidden size and
        hoping that it will be able to compensate the sequence that is
        dropped by the cell gate in the other hidden cell.
        \begin{figure}
            \begin{align*}
                &un \vert recogniz \vert able \\
                &inter \vert national \vert ities \\
                &oto \vert rhino \vert laryngolog \vert ical \\
                &hepatico \vert chol \vert angio \vert gastro \vert stomy
            \end{align*}
            \caption{Word examples with three or more subsequences}
            \label{fig:subsequence}
        \end{figure}

        \begin{figure}
            \begin{align*}
                unrecognizable : &unre \vert nrec \vert reco \vert ecog \vert cogn \vert ogni \vert gniz \vert niza \vert izab \vert\\ 
                &zabl \vert able\\
                internationalities : &inte \vert nter \vert tern \vert erna \vert rnat \vert nati \vert atio \vert tion \vert iona \vert\\
                &onal \vert nali \vert alit \vert liti \vert itie \vert ties\\
                otorhinolaryngological : &otor \vert torh \vert orhi \vert rhin \vert hino \vert inol \vert nola \vert olar \vert lary \vert \\
                &aryn \vert ryng \vert yngo \vert ngol \vert golo \vert olog \vert logi \vert ogic \vert gica \vert\\
                &ical\\
                hepaticocholangiogastrostomy : &hepa \vert epat \vert pati \vert atic \vert tico \vert icoc \vert coch \vert ocho \vert chol\\
                &hola \vert olan \vert lang \vert angi \vert ngio \vert giog \vert ioga \vert ogas \vert gast \vert\\
                &astr \vert stro \vert tros \vert rost \vert osto \vert stom \vert tomy
            \end{align*}
            \caption{4-grams examples}
            \label{fig:4grams}
        \end{figure}

        For all subsequence to be processed, a method that accounts
        for the whole sequence yet still able to divide the whole
        sequence into subsequences is needed. Consequently, n-grams
        was chosen because this method splits word into sequence of
        characters depending on the chosen window size as shown on
        figure \ref{fig:4grams}. Before processing the n-grams, the
        word first split into sequence of characters and then each
        character is transformed into embedding and processed with CNN
        inspired from CNN word n-grams \citep{convolutional2014kim}.
        Afterwards, those sequences of character embeddings were fed
        into learning algorithm. This idea is similar to how human
        tries to recognize an unseen word by reading subword that is
        understandable beforehand when no explanation or context was
        given. In other words, given a set of vocabulary $\mathcal{V}$
        with size $\vert\mathcal{V}\vert$ and pretrained embeddings
        $\mathcal{W}^{\vert\mathcal{V}\vert \times d}$ for each word
        $w_{i} \in \mathcal{V}$ that is represented as a vector $e_i$
        with $d$-dimension, the model is trained to map function
        $f:\mathcal{V} \rightarrow \mathbb{R}^d$ that minimizes the
        following loss function,
        \begin{equation}
            \label{eq:lossfn}
            \mathcal{L} = \Vert f(w_i) - e_i \Vert^{2}_2
        \end{equation}
        This approach is similar to \textsc{Mimick}
        \cite{mimicking2017Pinter}. The text input was represented as
        a sequence of character $[c_1, c_2, \dots, c_m]$ for $c_i \in
        \mathcal{C}$. Those sequence then transformed as sequence of
        vectors $g_i$ with $b$ dimension by using character embeddings
        $\mathcal{G}^{\vert \mathcal{C} \vert \times b}$. For
        simplicity, sequence of $[g_1, g_2, \dots, g_m]$ with length
        $m$ will be called $\{g\}^m$. $\{g\}^m$ becomes 2-dimensional
        matrix that has size of $m \times b$. In summary, the word
        $w$ was transformed using embedding generation function $h$
        into $\{g\}^m$ as follows,

        \begin{equation}
            \label{eq:word2charemb}
            h: w \rightarrow \{g\}_1^m
        \end{equation}

        To process $\{g\}^m$ like an n-grams, CNN n-grams
        implementation by \cite{convolutional2014kim} was used. CNN
        n-grams is basically a method to do convolution on matrix by
        using a kernel $k_i^{b \times n} \in K$ for $n$ is the window
        size of the grams and $b$ is the dimension size of the
        character embedding. This operation is represented with $*$
        symbol as stated in equation \ref{eq:conv_d:2}. This operation
        produced another vector $\hat{l}$ that represents the value of
        each grams, then non-linearity was applied to this vector by
        using ReLU activation function,
        \begin{equation}
            \label{eq:relu}
            ReLU(x) = max(0,x)
        \end{equation}

        Several kernels were used to learn several features for
        producing embedding. Each of these kernel was responsible to
        find grams that were affecting the results, thus the vector
        $\hat{l_i}$ that is results of convolution $\{g\}^m * k_i$
        will be maxpooled to produce one number. In details, from the
        given sequence of character embedding $\{g\}^m$, only gram
        that produces the highest value when convoluted by using
        kernel $k_i$ would be processed. Note that as mentioned
        earlier, each kernel $k_i$ can also return a high response for
        $j$ number of similar features from the input. Since, there
        are $\vert K \vert$ number of filter, $\vert K \vert + j$
        number of grams would be considered to be important for
        calculating the result. Furthermore, by using several window
        sizes for n-grams (bigram, trigram, etc.) by changing the size
        of the kernel more features will be able to be learned. For
        instance bigram has a kernel with size $b \times 2$, trigram
        has a kernel with size $b \times 3$, and so on and so forth,
        making the different sizes of n-grams can be trained together
        only then concatenated later.

    \subsection{Embedding Generation}
        After the features were able to be extracted, those features
        was concatenated and fed into fully connected layer with
        output size matching the pre-trained embedding $\mathcal{W}$
        with $d$-dimension by using non-linear activation function
        $Hardtanh$ matching the maximum and minimum bound of the
        pretrained embedding $\mathcal{W}$ resulting a new embedding
        vector $\tilde{e}$. The fully connected layer consists of one
        hidden layer and one output layer with batch normalization
        before each layer to further sped up the training convergence
        according to \cite{batchnorm:DBLP:journals/corr/IoffeS15}.
        This normalization layer basically maintains that the
        distribution of each minibatch remains similar so that
        covariance shift is minimized in the training process
        \citep{batchnorm:DBLP:journals/corr/IoffeS15}. This process
        was done by applying transformation on each minibatch to a
        certain mean and variance
        \citep{batchnorm:DBLP:journals/corr/IoffeS15}. This mean and
        variance are learnable parameters. Let minibatch $\mathcal{B}
        = {x_1, x_2, \dots, x_m}$, following operations were applied,
        \begin{align}
            \label{eq:bn:mu}
            \mu_{\mathcal{B}} &= \frac{1}{m} \sum_{i=1}^m x_i\\
            \label{eq:bn:var}
            \sigma^2_{\mathcal{B}} &= \frac{1}{m} \sum_{i=1}^m \left(x_i - \mu_{\mathcal{B}}^2\right)\\
            \label{eq:bn:norm}            
            \tilde{x}_i &= \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma^2_{\mathcal{B}} + \epsilon}}\\
            \label{eq:bn:transform}            
            BN(x_i) &= \gamma\tilde{x_i} + \beta
        \end{align}
        Equation \ref{eq:bn:mu} and equation \ref{eq:bn:var} tries to
        find the mean and variance to normalize the minibatch by
        equation \ref{eq:bn:norm}. After all of the data in the
        minibatch $\mathcal{B}$ were normalized, the data then scaled
        by parameter $\gamma$ and shifted by parameter $\beta$ as
        shown in equation \ref{eq:bn:transform}. By making the
        parameters $\gamma$ and $\beta$ learnable, the model will
        adjust these parameters in the training process.

        After all the process above was done, the generated embedding
        vector $\tilde{e}$ was produced.
        % then passed into a highway network to
        % decide whether some information should be carried or should be
        % forgotten to obtain a new sets of embedding. Given the output
        % of max-over-time pooling $m$, The highway network is
        % calculated by the following equation, 
        % \begin{align}
        %     \label{eq:highway}
        %     t &= ReLU(f(\mathbf{W}m + b))\\
        %     \mathbf{Z} &= t \odot g(\mathbf{W_{\mathbf{H}}}m + b_{\mathbf{H}}) + (1-t) \odot m
        % \end{align}
        The complete process from input word, feature extraction,
        until embedding prediction is shown on figure \ref{fig:model}.
        \begin{figure}
            \centering
            \includegraphics[width=.8\linewidth]{images/model_batchnorm.pdf}
            \caption{OOV Inferencing Model}
            \label{fig:model}
        \end{figure}
        On figure \ref{fig:model}, starting and ending token were
        added at the beginning and at the end of the word respectively.
        Furthermore, padding token was added if the input word was
        shorter than the longest input size in the minibatch. The
        padding token is a zero vector $\vec{0}$ and $\vec{0} * k =
        \vec{0}$ for any $k$. This is to ensure that part of input
        that got padded did not go through maxpool layer since only
        grams that has the highest value could go through the next layer
        and the result of $ReLU(0) = 0$, thus the result of maxpooling
        would not come from the padding token. The reason for this is
        so that various length of words were able to be processed by
        the model.

    \subsection{Error and Backpropagation}
        After the embedding $\tilde{e}$ was predicted, it was
        compared to the original embedding $e$ to adjust all of the
        parameters for the neural network by using mean squared error
        function,
        \begin{equation}
            \label{eq:errorf}
            Error = \frac{1}{2} \Vert e - \tilde{e} \Vert ^{2}_2
        \end{equation}
        By minimizing $Error$, it is similar to minimizing
        $\mathcal{L}$ shown in equation \ref{eq:lossfn}. The gradient
        of the $Error$ was backpropagated to fine-tune the neural
        network parameters, character embedding $\mathcal{G}$, the
        kernel $k \in K$, and the batch normalization parameters
        $\gamma$ and $\beta$.
        
\section{Measuring Performance on Downstream Tasks}
    In natural language modeling (NLP), there are several tasks that
    make use of word embedding. Hence that, the generated embeddings
    from the model can be evaluated by using those downstream tasks.
    Then the results from the downstream tasks were compared with the
    state-of-the-art OOV handling model \textsc{Mimick}
    \citep{mimicking2017Pinter}.
    
    \subsection{Part-of-Speech Tagging}
        Part-of-speech tagging or POS-tagging is a task of classifying
        word usage in a sentence or a corpus based on the grammatical
        usage of the word \citep{robustpostag2018horsmann}, for
        example: verb, noun, adverb, etc. The POS-tagger model is
        similar to the one that was proposed by
        \cite{finding2015ling}. Given sentence $S = \{w \in
        \mathcal{V} \vert [(w_1, t_1), (w_2, t_2), \dots, (w_n,
        t_n)]\}$ with its POS-tag $t_i$, each word $w_i$ that exist in
        the vocabulary $w_i \in \mathcal{V}$ and $w_i \in S$ was
        transformed into embedding $e_i$ by using the pre-trained
        embedding. For the OOV, every sequence of the characters
        building a word transformed into sequence of character
        embedding $\{g\}_{i}^m$ using the function represented in
        equation \ref{eq:word2charemb} then the embedding
        $\tilde{e}_i$ was predicted by using the OOV handling model.
        To separate embedding generation process for the in-vocabulary
        and the OOV, the output of the OOV handling model and the
        original embedding were masked in the following way,
        \begin{align}
            \label{eq:embeddingmask}
            embedding &= mask \odot e_i + (1-mask) \odot \tilde{e}_i\\
            mask &=
            \begin{cases}
                \vec{1} & \text{if }\ w_i \in \mathcal{V}\\
                \vec{0} & \text{otherwise}
            \end{cases}
        \end{align}        
        By doing so, the gradient would not flow into the OOV model if
        the word exist in $\mathcal{V}$ and would flow if the word is
        unavailable in $\mathcal{V}$ and the embedding was generated
        by the OOV model.

        The sequence of embeddings $\tilde{e}_i$ or $e_i$ then fed
        into bi-LSTM and the output was passed through LogSoftmax
        activation function,
        \begin{equation}
            \label{eq:logsoftmax}
            LogSoftmax(x_i) = log \Bigg(\frac{exp(x_i)}{\sum_j exp(x_j)}\Bigg)
        \end{equation}
        to classify the POS-tag $t$. To ease up computation time,
        adaptive LogSoftmax was used \citep{grave2018efficientsoftmax}.
        Instead of calculating the whole tag classification, the
        frequent and infrequent classes were separated, thus there were
        many chances that only the frequent classes needed to be calculated
        before trying to calculate the infrequent classes. After
        calculating the loss, stochastic gradient descent was used to
        optimize the parameters of the model. The complete process of
        POS-tagging process is shown in figure \ref{fig:postag}. After
        the training was done, the accuracy of the POS-tagger based on
        different OOV handling model were compared.

        \begin{figure}
            \centering
            \includegraphics[width=.8\linewidth]{images/postag.pdf}
            \caption{Pos-tagging Process}
            \label{fig:postag}
        \end{figure}
 
    \subsection{Word Similarity Tasks}
        Word similarity task is basically a task to evaluate the
        similarities between two words based on human given scores. In
        practice, several human subjects were given pairs of words and
        asked to score its similarities. Those scores then will be
        used to determine the agreements between subjects that certain
        word have stronger connection to a certain word than the
        other. In order to calculate the agreements between the OOV
        generated embedding and the data that is scored by human,
        Spearman's rank correlation coefficient is used. Firstly,
        given a pair $(w_1, w_2)$, the cosine distance of the
        embedding $\tilde{e}_1$ and $\tilde{e}_2$ based on the
        generated embedding from OOV model for $w_1$ and $w_2$ were
        calculated respectively using the following equation,
        \begin{equation}
            \label{eq:cosinesim}
            CosineSimilarity(e_1, e_2) = \frac{e_1 \cdot e_2}{\Vert e_1 \Vert \Vert e_2 \Vert}
        \end{equation}
        The OOV handling model was used if the word entry was an OOV and 
        the original embedding was used if the word entry was in-vocabulary.

        After all of the cosine distance for all pairs were calculated,
        Spearman rank's correlation from the dataset and the generated
        embedding were calculated by using equation \ref{eq:spearman}
        and by using equation \ref{eq:spearmantied} when no tied rank
        exists. The results of both \textsc{Mimick} and the proposed
        model from several word similarity datasets then averaged and
        compared.
        
        \begin{align}
            \label{eq:spearman}
            \rho    &= \ddfrac{n\sum_{i=1}^n u_i v_i - \Bigg( \sum_{i=1}^n u_i \Bigg) \Bigg( \sum_{i=1}^n v_i \Bigg)}{\sqrt{\Bigg[ n \sum_{i=1}^n u_i^2 - \Bigg( \sum_{i=1}^n u_i \Bigg)^2 \Bigg] \Bigg[ n \sum_{i=1}^n v_i^2 - \Bigg( \sum_{i=1}^n v_i \Bigg)^2 \Bigg] }}\\
            \label{eq:spearmantied}            
                    &= 1 - \ddfrac{6 \sum_{i=1}^n d_i^2}{n(n^2 - 1)}\ \text{where}\ d_i = u_i - v_i
        \end{align}
