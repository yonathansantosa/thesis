\chapter{Conclusion and Future Works}
\label{chap:conc}

Given some training examples of pretrained word embedding, machine
learning can be used to generate embedding for
\textit{out-of-vocabulary} embedding which unavailable in the
pretrained word embedding vocabulary collection. This method however,
needs time and resources to be trained to generate a good
approximation of the \textit{out-of-vocabulary} embedding.

\section{OOV Handling Model against Randomly Initialized OOV Embedding}
For \textit{out-of-vocabulary} embedding to be used in the downstream
tasks, one can assign randomly generated embedding for each
\textit{out-of-vocabulary} or by assigning single token with one
vector representation replacing all \textit{out-of-vocabulary}
entries. This approach has its advantages such as cheaper computation time
to generate embedding and has relatively high accuracies at around
$91\%$ for POS-tagging task. However, the training on the downstream
tasks using randomly generated embedding might need more epoch or more
sophisticated architecture to be able to achieve similar accuracy with the one that
uses OOV handling model such as \textsc{Mimick} or the proposed model
from this research. As evidence, with similar setup as the OOV
handling model the randomly generated embedding for
\textit{out-of-vocabulary} has lower accuracy for POS-tagging tasks.
The randomly generated embedding for \textit{out-of-vocabulary}
entries method could produce POS-tagging results for three different
pretrained embedding averaged around $91\%$ and by using OOV handling
model the accuracy increased to be around $93\%$.

\section{CNN against LSTM for Extracting Text Features}
As shown by the results of the downstream tasks, CNN model generally
outperforms bi-LSTM implemented in \textsc{Mimick}. As mentioned before,
\textsc{Mimick} uses the last state of the hidden state from the
bi-LSTM architecture. This renders the intermediate sequence that
could be important for predicting the embedding lost when the cell
gate decided to drop previous information. By using CNN to extract
text features, the intermediate sequence that appears somewhere in the
middle of a word can still be inferred. With some generalization, the
CNN features also able to use one kernel for several sequence that has
certain similarities in terms of sequence of embeddings, hence the
higher accuracy and Spearman's rank correlation values.

\subsection{POS-tagging}
In POS-tagging tasks, CNN model accuracy was averaged around 0.2\% higher than
\textsc{Mimick} across all settings. On top of the CNN architecture, some batch
normalization were added to make the training converge faster. The
only disadvantages of the CNN model is that it has more parameters to
learn, meaning higher complexity and slower training time. On top of
that, \textsc{Mimick} seems to perform better \textit{out-of-the-box}
compared to CNN. The advantages of the CNN model is that it perform
better than \textsc{Mimick} when the OOV handling model is included in
the training.

\subsection{Word Similarity Task}
Word similarity task was used to evaluate the distance between two
words according to human subject. In this task, the word generated
embedding expected to have similar degree of similarities between two
embeddings from two words. In this task, the CNN model were able to
achieve higher $\rho$ value than \textsc{Mimick}. Even though this
task is highly dependent on the embedding used for predicting the
embedding of given word since every embedding is trained on different
corpus and trained with different purposes. Despite of that, CNN still
able to performs better than \textsc{Mimick}.

\section{Future Works}
Both models \textsc{Mimick} and CNN were able to produce embeddings
for \textit{out-of-vocabulary} words, but both model are not really
able to fully define a function that could generate word embedding
mimicking existing pretrained embedding since updating pretrained
embedding with new dataset will requires retraining the embedding.
Instead of retraining the embedding, finding function or model that
are able to mimick the pretrained embedding will save computation time
at least until the language itself is changing into something that is
completely different making the word embedding becomes obsolete. On
top of that, if the function can generate the in-vocabulary embedding,
it can save storage capacity since in general pretrained embedding
storage size is large.