\chapter{Related Works}
\label{chap:related}

% This is the introduction to the thesis.\footnote{And this is a
% footnote.}  The conclusion is in Chapter on page
Word2vec is one of word embedding that is trained using skip-gram
model \{CITE\}. A word $w(t)$ used as an input and its context word, for
example context word with windows of 4 $w(t-2), w(t-1), w(t+1),$ and
$w(t+2)$, used as the target and the projection from input to the
output is used as the embedding of the input $w(t)$. This model is
highly dependant on the corpus completeness. More examples and
vocabulary a corpus has the better the representation of the embeddings.

postagging explanation

mimick explanation

cnn-grams approach explanation