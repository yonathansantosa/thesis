\chapter{Introduction}
\label{chap:intro}

% This is the introduction to the thesis.\footnote{And this is a
% footnote.}  The conclusion is in Chapter on page.
\section{Motivation} 
    Word embeddings is a method of word representation mainly used for
    natural language processing. Text data appears differently in
    computer from images and sounds. Generally image represented as
    two-dimensional matrix with finite number of points cell elements
    containing numerical value \citep{imageprocessing2018tyagi}. On the
    other hand, sound is represented as one-dimensional signal
    representing air pressure in the ear canal
    \citep{sound1995rocchesso}. Both images and sounds can be easily
    represented as mathematic models either using analog or digital
    signals but not with text data \citep{wordembedding2017yang}. Text
    data consists of strings that can only be modeled using one-hot
    vector (from given \textit{d}-dimension for \textit{d} words that
    are known, only one dimension is one and the rest are zeroes).
    This one-hot vector does not have any information that infers
    connection between one to the other. Hence that, vector
    representations that maps semantic and syntactic information given
    one-hot vectors in a euclidean space is introduced
    \citep{wordembedding2017yang}. This positional information then
    can be used to infer interconnection between words, whether its
    similarities or usage of the word in a sentence
    \citep{distributional1954harris}. To obtain word embeddings, a
    model is created to extract features of a word from a corpus and
    map its location in euclidean space based on the features found.
    These word embeddings then can be used to do many downstream
    tasks, such as postagging, named entity recognition (NER), and
    sentiment analysis \citep{finding2015ling, neural2016lample}. In
    general, large corpus with many words and examples of word usage
    is prefered because the size of the vocabularies will increases
    and more words connection can be inferred from the corpus
    \citep{size2018kutuzov}. However, it is not posible to have such
    corpus since the language itself is changing overtime
    \citep{forrester2008abrief}. Furthermore, there are cases of
    typographical error, especially after internet and social media
    are booming where anybody willingly write text over these
    platforms and these words may not be present in the corpus hence
    not included in the vocabulary thus making many typographical
    error while in its correct form it actually is. In addition with
    the increased number of smartphone which uses touch screen, some
    typographical error is expected \citep{ghosh2017correction}. All
    this words that are non-existent in the corpus and its embedding
    cannot be inferred are called as \textit{out-of-vocabulary} (OOV)
    words. One may use simple approach by asigning unique random
    embedding or by replacing OOV with an unknown \textit{\textless
    UNK\textgreater} embedding. While in some cases using these simple
    approaches and continue on the training on the downstream tasks
    can produce acceptable results \{CITE\}, further improvement on
    downstream tasks can be achieved by using machine learning method
    to infer OOV embeddings.

    To infer OOV embeddings, the model is built over quasi-generative
    perspective. Only knowing the vocabularies and its embedding, the
    model tried to generate embedding for OOV words. Previous
    \textit{state-of-the-art} used lstm to infer OOV embeddings
    \citep{mimicking2017Pinter}. Character embedding is used to
    transform sequence of characters then forwarded into bi-lstm then
    to fully connected layer. In language model, bi-lstm generally
    works by separating sub-word by remembering and forgeting previous
    sequence from both end. Those sub-word then will be used to infer
    its embedding. The problem might arise when there are more than
    two important sub-words and they are not in sequence, meaning that
    there exist at least one character between two sub-words
    considered important, hence the information is incomplete since
    lstm will dampen the previous sequence if the next sequence
    sub-words are considered more important. Because of the
    aformentioned problem, a new method will be proposed.

\section{Objectives}
    Instead of taking the whole words as a sequence and considering
    its importance based on time using LSTM, by using n-grams and
    picking which grams that are considered to be important in theory
    should gives better results since the information is complete and
    only left for the model to pick which n-grams features are more
    important. The only problem is that for the model to pick which
    grams that should be included in the model is impossible to do
    since there are many word combinations making the model needs to
    accept huge number of inputs. Instead of handpicking the features
    of n-grams, convolutional neural network (CNN) can be used to pick
    which features needs to be considered by using character embedding
    and treat the character sequence embedding as an image. The
    features picked then will be processed to predict the word
    embedding for the input word.

\section{Contributions}
    \begin{enumerate}
        \item A new OOV handling model
        \item Improved OOV handling model
    \end{enumerate}

\section{Thesis Structure}
    The remainder of this document is structured as follows. In the
    \nameref{chap:relatedwork} chapter, the previous works that are in
    relatives with research done in this documents are mentioned
    especially the baseline used in this research. In the following,
    \nameref{chap:preliminaries} chapter, base theories that
    considered to be needed are explained in details here. In the
    \nameref{chap:method} chapter, the method of solution proposed to
    the problem are explained. In the \nameref{chap:implementation}
    chapter, the method of solution proposed to the problem are
    described in technical way to show how it is implemented. In the
    \nameref{chap:results} chapter, the results are shown and
    discussed the relation with the baseline. Lastly,
    \nameref{chap:conc} chapter talks about the conclusion that are
    able to pull from this research.


% \section{About the logo}

% Figure \ref{us_figure} shows the logo for the University of
% Sussex.\footnote{This is a URL: \url{http://www.sussex.ac.uk}} This
% is consistent with Special Relativity \citep{Einstein1905}.
% $E=mc^2$.

% \begin{figure} \centering \includegraphics[width=5cm]{uslogo}
%     \caption{The logo for the University of Sussex.}
%     \label{us_figure} \end{figure}

% Here is some Latin.

% Class aptent taciti sociosqu ad litora torquent per conubia nostra,
% per inceptos hymenaeos. Nullam suscipit lectus nec tellus. Praesent
% malesuada nisl in neque. Nam dictum semper nisl. Ut ultricies
% nonummy augue. Nunc ullamcorper eros in nisl. Sed eros purus,
% vehicula eget, mattis nec, dignissim non, nulla. Suspendisse ac est
% commodo libero rhoncus sagittis. Vestibulum quis augue ut enim
% tincidunt ullamcorper. Vestibulum sem. Pellentesque enim ligula,
% consequat quis, luctus vel, rutrum id, sem.

% Donec eleifend erat quis enim. Maecenas volutpat cursus libero.
% Fusce velit. Duis in metus. Sed lobortis, lorem id molestie
% ullamcorper, leo lacus interdum urna, a dapibus augue massa id
% magna. Curabitur leo. Cras sit amet lorem ut massa tincidunt
% ullamcorper. Nulla sed urna vulputate enim sodales pharetra. Cras
% vitae nulla a diam aliquam fermentum. Donec ullamcorper porttitor
% arcu. Ut laoreet est. Suspendisse potenti. Curabitur tincidunt,
% lorem nec pharetra viverra, arcu tellus tincidunt metus, sed
% fringilla ligula lorem at odio. Integer arcu turpis, facilisis quis,
% rhoncus quis, tristique et, nunc. Aenean massa pede, tempus nec,
% sodales at, tristique id, tortor. Aenean porttitor, sapien et
% interdum eleifend, urna felis eleifend nisi, non sagittis justo erat
% et lorem.

% Nam risus. Curabitur nec lectus. Nullam lobortis lacinia ipsum.
% Donec sit amet tortor id sem tincidunt congue. Praesent ut quam. Sed
% nisl nulla, adipiscing sit amet, dapibus ut, rutrum et, massa. Nunc
% fringilla tincidunt nisl. Vestibulum vehicula nisl id augue. Sed
% lobortis ligula sit amet nulla. Suspendisse viverra mauris non
% libero. Curabitur ac neque at lectus consectetuer tempus. Donec
% molestie magna consequat quam. Donec placerat turpis et risus.
% Integer purus purus, accumsan sed, euismod eget, commodo ornare,
% velit. Duis sit amet augue ut velit tristique blandit. Morbi in
% odio. Nam urna.

% Curabitur pulvinar tristique pede. Duis justo. Morbi libero diam,
% varius et, faucibus non, blandit sit amet, nisi. Nam quam nunc,
% mattis id, scelerisque vel, pellentesque at, nunc. Etiam mattis
% ultrices odio. Suspendisse aliquam nisi sed sem. Praesent
% scelerisque ultrices velit. Nam sit amet lectus. Nullam in ipsum vel
% lectus nonummy consectetuer. Sed dictum. Maecenas massa sapien,
% blandit in, sollicitudin id, vulputate ut, risus. Nulla facilisi.
% Vivamus ut erat. Etiam massa. Cum sociis natoque penatibus et magnis
% dis parturient montes, nascetur ridiculus mus. Suspendisse hendrerit
% iaculis mi. Morbi tincidunt felis a urna.

% Ut a quam. Nam aliquet suscipit pede. In vitae magna. Aliquam erat
% volutpat. Etiam ut turpis. Ut convallis adipiscing velit. Cum sociis
% natoque penatibus et magnis dis parturient montes, nascetur
% ridiculus mus. Nam luctus ante sit amet pede. Nulla facilisi.
% Pellentesque habitant morbi tristique senectus et netus et malesuada
% fames ac turpis egestas. Etiam ultricies augue non eros. Vivamus
% nulla lacus, varius sed, consectetuer id, tempus sed, velit.
% Pellentesque habitant morbi tristique senectus et netus et malesuada
% fames ac turpis egestas. Phasellus non dolor eu augue adipiscing
% molestie. Duis eu est. Proin sodales pellentesque quam. Duis at
% enim. Nulla vulputate, diam sed rutrum euismod, quam mauris
% consectetuer massa, a dapibus nisi eros ac ligula. Nam tortor metus,
% faucibus vitae, cursus vel, egestas sollicitudin, nulla.
% Pellentesque habitant morbi tristique senectus et netus et malesuada
% fames ac turpis egestas.