\chapter{Introduction}
\label{chap:intro}

% This is the introduction to the thesis.\footnote{And this is a
% footnote.}  The conclusion is in Chapter on page.
\section{Motivation} 
    Word embedding is a method for word representation mainly used in
    natural language processing domain. However, text data represented
    differently in machine unlike images and sounds data. Generally
    image is represented as two-dimensional to four-dimensional (given
    channels and alpha value) matrix with finite number of cell
    elements containing numerical value to represent color intensity
    on each location \citep{imageprocessing2018tyagi}. For example RGB
    image represented with 3-dimensional matrix. Each dimension
    represents the intensity of red color, green color, and blue color
    respectively in form of two-dimensional matrix. On the other hand,
    sound is represented as one-dimensional signal or stack of those
    signals (given several channels, it becomes two-dimensional)
    representing air pressure in the ear canal (for instance one
    channel for the left ear and one for the other)
    \citep{sound1995rocchesso}. Both images and sounds can be easily
    represented as mathematical models either using analog or digital
    signals but not with text data \citep{wordembedding2017yang}.
    Hence word embedding was introduced to give ability in
    representing text as a mathematical model namely a vector.
    
    Text data consists of sequence of characters that is represented
    by codes that is standardized, for example ASCII. In ASCII each
    character is represented by a number from $0$ to $127$ that later
    on extended until $255$. Combinations of these number then
    translated by computer to represent a character. Originally, in
    natural language processing text data can only be modeled using
    one-hot vector. This vector is a one-dimensional vector that has
    $d$-dimension, given \textit{d} words that are known or used. Each
    word is represented by one dimension and depends on the used word
    entries in the sentence, the correspondent dimension's value is
    $1$ and the rest is $0$ hence the name one-hot vector. The one-hot
    vector then stacked with another one-hot vectors to represent
    order of use in a sentence. Similar representation is also used to
    represent characters. The problem with one-hot vector is that it
    does not have any information that infers connection between one
    to another. It only encodes that certain word is used in a certain
    sentence in a certain sequence. Instead of sparse representation
    for each word, dense vector representations that maps semantic and
    syntactic information between words given one-hot vectors in a
    Euclidean space is introduced \citep{wordembedding2017yang,
    Distributed2013mikolov}. Hopefully, this positional information
    can be used to infer interconnection between words, whether its
    similarities or usage of the word in a sentence
    \citep{distributional1954harris}. To obtain word embeddings, a
    model is created to extract features of a word from a corpus and
    map its location in Euclidean space based on the features found
    \citep{Distributed2013mikolov, polyglot2013alrfou,
    dict2vect2017tissier}. These word embeddings then can be used to
    do many downstream tasks, such as POS-tagging, named entity
    recognition (NER), and sentiment analysis \citep{finding2015ling,
    neural2016lample}. 

    In general, large corpus with many words and examples of word
    usage is preferred because the size of the vocabularies will be
    higher and more words connection can be inferred from the corpus
    \citep{size2018kutuzov}. On top of that, multiple use of a
    vocabulary might also be used as a training data as an examples of
    cases of vocabulary usage in a sentence. However, it is not
    possible to have enough corpus since the language itself is
    creative and changing overtime \citep{forrester2008abrief,
    speech2009Jurafsky:2009:SLP:1214993}. Furthermore, there are cases
    of typographical error, especially on social media platform where
    anybody willingly write text over these platforms
    \citep{Liu2010SentimentAA} and some of these words may not be
    present in the corpus hence not included in the vocabulary even
    though there is another word that has the exact similar meaning to
    those words thus it should more or less has close distance to the
    standard or original word \citep{mapping2012eisenstein}. In
    addition with the increasing number of smartphones which uses
    touch screen, increase of number in typographical error is to be
    expected \citep{ghosh2017correction}. All these non-existent words
    in the corpus because of inability to collect such corpus or
    simply because of emergence of new slang or typographical error
    making the embedding of such word is unknown and it is called as
    \textit{out-of-vocabulary} (OOV) words. One may use simple
    approach by assigning unique random embedding for every OOV or by
    replacing OOV with an unknown \textit{\textless UNK\textgreater}
    token with randomly initialized embedding
    \citep{predicting2019garneau}, that later hoped to be generalized
    in training. Despite the fact that it can be used for OOV, further
    improvement on downstream tasks should be able to be achieved by
    using machine learning method to infer OOV embeddings.

    To infer OOV embeddings, the proposed model will be built over
    quasi-generative perspective. Only knowing the vocabularies and
    its embedding, the embedding for OOV words will be generated by
    the model. Previous \textit{state-of-the-art} used LSTM to infer
    OOV embeddings called \textsc{Mimick} \citep{mimicking2017Pinter}.
    For this model to infer OOV embedding, character embedding was
    first randomly initialized with a set of characters as its
    vocabulary. The character embedding then used to transform
    sequence of characters in a word into sequence of embedding. The
    sequence of the character embeddings then forwarded into
    bidirectional Long-Short Term Memory (bi-LSTM) then to fully
    connected layer. In language model, bi-LSTM generally works by
    separating sub-word by remembering and forgetting previous
    sequence from both end. How LSTM works in connection to this
    problem will be further explained in chapter \ref{chap:method}.
    The last hidden state of the bi-LSTM then will be used to infer
    its embedding. By architecture of LSTM, the gates inside the
    hidden neuron might drop previous information. Hence a problem
    might arise when there are more than two important sub-words and
    they are not in sequence, meaning that there exist at least one
    character between two sub-words considered important, hence the
    information is incomplete since the previous information will be
    dampened by LSTM hidden neuron interiors if the next sequence of
    sub-words are considered more important. This problem will be
    explained further in chapter \ref{chap:method}. From the
    explanation above, proposal of a new method to handle OOV is
    created.

\section{Objectives}
    For OOV to be inferred, a model that are able to generate
    embedding for the correspondence word has to be created. In
    \textsc{Mimick}, the whole sequence is processed by a bi-LSTM. By
    the problem mentioned earlier, instead of taking the whole words
    as a sequence and considering its importance based on time and
    occurrence using bi-LSTM, n-grams will be used to pick which grams
    (set of sequence) that are considered to be important. In theory
    this method should gives better results in downstream tasks since
    the information fed is complete and only left for the model to
    pick which n-grams features are more important. The only problem
    is that for the model to pick which grams that should be included
    in the model is impossible to do since there are many word
    combinations making the model needs to accept huge number of
    inputs. Instead of handpicking the features of n-grams,
    convolutional neural network (CNN) can be used to learn the
    existed features and pick which features needs to be considered by
    using character embedding and treat the character sequence
    embedding as two-dimensional matrix. The features picked then will
    be processed to predict the word embedding for the input word
    using feedforward network.

\section{Contributions}
    \begin{enumerate}
        \item An improved OOV handling model for downstream task
        \item Evaluation on different settings for baseline model and
        the proposed model
    \end{enumerate}

\section{Thesis Structure}
    The remainder of this document is structured as follows. In the
    \nameref{chap:relatedwork} chapter, the previous works that are in
    relatives with research done in this documents are mentioned
    especially the baseline used in this research. In the following,
    \nameref{chap:preliminaries} chapter, base theories for
    feedforward neural network, recurrent neural network, and n-grams
    that are considered to be needed are explained in details here. On
    top of that, the problem of the previous \textit{state-of-the-art}
    will be explained in depth here. In the \nameref{chap:method}
    chapter, the method of solution proposed to the problem and the
    testing method for analyzing the results are explained. In the
    \nameref{chap:implementation} chapter, the method of solution
    proposed to the problem are described in technical way to show how
    it is implemented and tested. In the \nameref{chap:results}
    chapter, the results are shown and discussed with the relation
    with the previous \textit{state-of-the-art}. Lastly,
    \nameref{chap:conc} chapter talks about the conclusion that are
    able to be pulled from this research.