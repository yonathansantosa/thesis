\chapter{Introduction}
\label{chap:intro}

% This is the introduction to the thesis.\footnote{And this is a
% footnote.}  The conclusion is in Chapter on page.
\section{Motivation} 
    Word embedding is a numerical form for word representation which
    is mainly used in natural language processing domain. Unlike
    images and sounds data, text data is represented differently in
    machine. Generally, image is represented as two-dimensional to
    four-dimensional matrix (given channels and alpha value) with
    finite number of cell elements containing numerical value to
    represent color intensity on each location
    \citep{imageprocessing2018tyagi}. For example, RGB image is
    represented with 3-dimensional matrix. Each channel represents the
    intensity of red color, green color, and blue color respectively
    in the form of 2-dimensional matrix. On the other hand, sound is
    represented as one-dimensional signal or a stack of those signals
    (given several channels, it becomes two-dimensional) that
    represents air pressure in the ear canal (for instance one channel
    for the left ear and one for the other)
    \citep{sound1995rocchesso}. Both images and sounds can be easily
    represented as mathematical models either by using analog or
    digital signals but not with text data
    \citep{wordembedding2017yang}. Hence word embedding was proposed
    to give the ability to represent text as a mathematical model
    namely a dense vector.
    
    Text data consists of a sequence of characters that is represented
    by standardized codes, for example ASCII. In ASCII each character
    is represented by a number from $0$ to $127$ that later on
    extended until $255$. Combinations of these number are translated
    by computer to represent a character. Originally, text data can
    only be modeled by using one-hot vector in natural language
    processing. This vector is a one-dimensional vector that has
    $d$-dimension, given \textit{d} words that are existed or used.
    Each word is represented by one dimension and depending on the
    used word entries in the sentence, the correspondent dimension's
    value is $1$ and the rest is $0$, hence the name one-hot vector.
    The one-hot vector is stacked with another one-hot vectors to
    represent the order of use in a sentence. Similar representation
    is also used to represent characters sequence. The problem with
    one-hot vector is the lack of any information that infers
    connection between one to another. It only encodes that a certain
    word is used in a certain sentence in a certain sequence. Instead
    of a sparse representation for each word, dense vector
    representation that maps semantic and syntactic information
    between words given one-hot vectors in a Euclidean space is
    introduced \citep{wordembedding2017yang, Distributed2013mikolov}.
    Hopefully, this positional information can be used to infer
    interconnection between words, whether its similarities or usages
    of the word in a sentence \citep{distributional1954harris}. To
    obtain word embeddings, a model is created to extract features of
    a word from a corpus and map its location in Euclidean space based
    on the features found \citep{Distributed2013mikolov,
    polyglot2013alrfou, dict2vect2017tissier}. These word embeddings
    then can be used to do many downstream tasks, such as POS-tagging,
    named entity recognition (NER), and sentiment analysis
    \citep{finding2015ling, neural2016lample}.

    In general, large corpus with many words and examples of word
    usage is preferred because the size of the vocabularies will be
    larger and more connection between each word can be inferred from
    the corpus \citep{size2018kutuzov}. On top of that, multiple usage
    of a vocabulary might also be used as a training data as an example
    of vocabulary usage in a sentence. However, it is not possible to
    have enough corpus since the language itself is creative and
    changing overtime \citep{forrester2008abrief,
    speech2009Jurafsky}. Furthermore, there are cases
    of typographical error, especially on social media platform where
    anybody willingly write a text over these platforms
    \citep{Liu2010SentimentAA} and some of these words may not be
    exist in the corpus hence not included in the vocabulary even
    though there is another word which has the exact similar meaning to
    those words thus it should have more or less a close distance to the
    standard or original word \citep{mapping2012eisenstein}. In
    addition, with the increasing number of smartphones which uses
    touch screen to type, the number of typographical error is
    expected to be increased as well \citep{ghosh2017correction}. All
    these unavailable words in the corpus because of the inability to
    collect such corpus or simply because of the emergence of the new slang or
    the typographical error which makes the embedding of such word unknown
    is called as \textit{out-of-vocabulary} (OOV) words. One
    may use a simple approach by assigning a unique random embedding for
    every OOV or by replacing OOV with an unknown ``\textit{\textless
    UNK\textgreater}'' token with randomly initialized embedding
    \citep{predicting2019garneau}, that later expected to be generalized
    in training. Despite the fact that it can be used to represents
    OOV, a further improvement on downstream tasks should be able to be
    achieved by using machine learning method to infer OOV embeddings.

    To infer the OOV embeddings, the proposed model would be built
    over quasi-generative perspective. Only by knowing the pre-trained
    vocabularies and its embedding, the model would generate embedding
    for OOV words. Previous \textit{state-of-the-art} used
    bi-directional long-short term memory (bi-LSTM) to infer OOV
    embeddings which called \textsc{Mimick} \citep{mimicking2017Pinter}. For
    this model to infer OOV embedding, character embedding was first
    randomly initialized with a set of characters as its vocabulary.
    The character embedding was used to transform sequence of
    characters in a word into sequence of embedding. The sequence of
    the character embeddings was forwarded into bi-LSTM then to fully
    connected layer. In a language model, bi-LSTM generally works by
    separating sub-word by remembering and forgetting previous
    sequence from both direction. 
    
    In the implementation of \textsc{Mimick}, the last hidden state of
    the bi-LSTM was used to predict its embedding. By the architecture of
    LSTM, the gates inside the hidden neuron might drop the previous
    information if the input triggers the cell gate to do so. Hence a
    problem might arise when there are more than two important
    sub-words or subsequences which are not in sequence, meaning
    that there exist at least one character that could trigger the
    cell gate to drop previous information between two sub-words that
    considered to be important by the model. Because of that, the
    information in the last hidden state is incomplete since the
    previous information will be dampened or completely dropped by
    LSTM hidden neuron interiors if the next subsequence of the word
    is considered to be important. This problem will be explained
    further in chapter \ref{chap:method}. From the explanation above,
    proposal of a new method to handle OOV is created.

\section{Objectives}
    For OOV to be inferred, a model that is able to generate embedding
    for the correspondence word has to be created. In \textsc{Mimick},
    the whole sequence was processed by a bi-LSTM. By the problem
    mentioned earlier, instead of taking the whole words as a sequence
    and considering its importance based on time and occurrence by
    using bi-LSTM, n-grams will be used to pick which grams (set of
    sequence) that are considered to be important. In theory this
    method should give better results in downstream tasks since the
    information that is processed is complete and only left for the
    model to pick which n-grams features are more important. What is
    left is for the model is to pick which grams that should be used
    in the model which is impossible to do since there are many word
    combinations which makes the model needs to accept a huge number
    of inputs. Instead of handpicking the features of n-grams,
    convolutional neural network (CNN) can be used to learn the
    existing features and pick which features need to be considered
    important by using character embedding and treat the character
    sequence embedding as two-dimensional matrix. CNN has kernels that
    define which feature is needed by training the model. On top of
    that, the features that are closely similar to the needed feature
    will still be able to produce high response. The features that are picked
    will then be processed to predict the word embedding for the input
    word by using feedforward network.

\section{Contributions}
    \begin{enumerate}
        \item A new OOV handling model that used CNN for subword learning
        \item Evaluation on OOV handling by using random embedding or 
        unknown token ``\textless UNK\textgreater'' against machine learning
        method for OOV handling
        \item Analysis of the baseline model and the proposed model
        regarding on the OOV and the in-vocabulary embeddings
        \item Evaluation on different settings for baseline model and
        the proposed model against downstream tasks
    \end{enumerate}

\section{Thesis Structure}
    The remainder of this document is structured as follows. In the
    \nameref{chap:relatedwork} chapter, the previous works that are in
    relatives with the research done in these documents are mentioned,
    especially the baseline used in this research. In the following,
    \nameref{chap:preliminaries} chapter, base theories for
    feedforward neural network, recurrent neural network, and n-grams
    that are considered to be needed are explained in details here. On
    top of that, the problem of the previous \textit{state-of-the-art}
    will be explained in depth here. In the \nameref{chap:method}
    chapter, the method of the solution proposal to the problem and
    the testing method for the results analysis are explained. In the
    \nameref{chap:implementation} chapter, the method of solution
    proposed to the problem are described in a technical way to show how
    it is implemented and tested. In the \nameref{chap:results}
    chapter, the results are shown and discussed to the relation to
    the previous \textit{state-of-the-art}. Lastly,
    \nameref{chap:conc} chapter talks about the conclusion that are
    able to be pulled from this research.