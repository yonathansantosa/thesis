\chapter{Implementation}
\label{chap:implementation}

\section{Datasets}
    \subsection{Pretrained Word Embedding}
        In order to train the model, pretrained embedding is needed
        since the model will tries to predict embedding of from known
        vocabulary $v \in \mathcal{V}$ with its known embedding $w \in
        \mathcal{W}$. For this purpose, word2vec which trained on
        Google news dataset using skip-gram model containing 3 million
        words and phrases \cite{Distributed2013mikolov}. Word2vec
        contains phrases from negative sampling and for the purpose of
        this research those phrases did not included. The embedding
        has 300 dimensional vectors. Only top 40 thousands words with
        removal if the word is actually a phrase.
        
        Another pretrained embedding is polyglot
        \cite{polyglot2013alrfou} which contains multilingual
        embeddings. For this research, only English embedding that
        contains around 100 thousands with 60 dimensional vector
        representation. This pretrained embedding is also used in OOV
        handling model \textsc{Mimick} \cite{mimicking2017Pinter}.
        This model is used as baseline model.

        Dict2vec is yet another word embedding that trained based on
        the definition of a word in dictionary
        \cite{dict2vect2017tissier}. Originally, this embedding was
        tested using word similarity tasks, hence this embedding is
        used as baseline model for word similarity tasks.

    \subsection{Word Similarity Dataset}
        Several word similarity dataset were used in order to increase
        the pair examples since for the datasets that is collected,
        the maximum number of pairs is 3000 pairs. Those datasets are
        asdf, sadf, asdf, asdf, asdf, asdf, sadg, and asdfsadf.

\section{Programming Language and Tools}
    The model was using PyTorch 1.0.1
    \cite{pytorch2017paszke} on top of Python 3.6. Most of the
    basic model, for instance 2d convolution layer, 2d maxpool
    layer, fully connected layer, bi-lstm, and many activation
    functions and loss function, thus will serve enough for the
    purpose of this research.
        
\section{Hardware}
    The model was trained on the freely available google colaboratory
    (links here) which gives randomized hardware specification, thus
    the exact hardware used cannot be determined. Nevertheless, this
    only affects the time needed to train the model not the results.