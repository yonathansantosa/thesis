\chapter{Related Works}
\label{chap:related}

% This is the introduction to the thesis.\footnote{And this is a footnote.}  The
% conclusion is in Chapter on page
Word embeddings is a method of word representation mainly used for natural
language processing. It consists of vocabularies and its positional information
in \textit{d}-dimensional space. This positional information then can be used to
infer interconnection between words, whether its similarities or usage of the
word in a sentence\{CITE\}. To obtain word embeddings, one must create some kind of
model to extract features of a word from a corpus. In general, large corpus with
many words and examples of word usage is prefered because the size of the
vocabularies will increases and more words connection can be inferred\{CITE\}. However,
it is not posible to have such corpus since the language itself is evolving and
there are many cases of typography in a documents and these words maybe not
present in the corpus hence not included in the vocabulary. We called this words
as \textit{out-of-vocabulary} (OOV) words. Some simple approach to handle OOV
words are by asigning a random embeddings or by asigning an unknown
\textit{\textless UNK\textgreater} embedding\{CITE\}. While in some cases using these
simple approaches can produce acceptable results \{CITE\}, we can further
improve it by using machine learning method to infer OOV embeddings for
downstream tasks.

To infer OOV embeddings, we look it from a quasi-generative perspective. Only
knowing the vocabularies and its embedding, we tried to generate embedding for
OOV words, then we tested the results for downstream task namely postagging.